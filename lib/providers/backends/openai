#!/usr/bin/env sh

# NAME
#  openai-send - Interface for OpenAI's /chat/completions endpoint.
#
# SYNOPSIS
#  openai-send
#
# DESCRIPTION
#  Low-level script calling the OpenAI API to get completions for a given
#  message chain.
#
#  See the OpenAI API reference for more details:
#  https://platform.openai.com/docs/api-reference/chat
#
# STDIN
#  JSON array of objects with `role` and `content` fields.
#
# OPTIONS
#  -m|--model <model>
#   The model to use for generating completions.
#
#  -mt|--max-tokens <max_tokens>
#   The maximum number of tokens to generate in the completion.
#
#  [-t|--temperature <temperature=0.7>]
#   The sampling temperature to use when generating completions.
#   Higher values mean the model will take more risks.
#
#  [--no-stream]
#   Disable streaming response.
#
#  [--raw]
#   Output raw response from OpenAI without any processing
#
# ENVIRONMENT
#  OPENAI_API_KEY
#   The API key used for authenticating with the OpenAI API. 
#
#  OPENAI_ORG_ID
#   The organization ID used for authenticating with the OpenAI API.
#
# ERROR CODES
#  1: Missing environment variables
#  2: Invalid input - parsing error of stdin, argument or flag parsing
#  3: CURL error - networking or API error
# 
# SEE ALSO
#  curl(1), jq(1)

if [ -z "$OPENAI_API_KEY" ]; then
  log error -v var_name "\$OPENAI_API_KEY" "Missing environment variable"
  exit 1
fi

# ╭───┤ Bootstrap
# ╰─

export LOG_NAMESPACE="$LOG_NAMESPACE.openai-send"

# shellcheck source=../../_core/fn/trap-debug
. "$SH41_LIB/_core/fn/trap-debug"
# shellcheck source=../../_core/fn/validation-utils
. "$SH41_LIB/_core/fn/validation-utils"

# ╭───┤ Functions
# ╰─

# Extract the content from the streamed response chunks.
# - When streaming, OpenAI sends multiple JSON objects with the content of the
# completion.
# - Each chunk contains the string "data: {...}" followed by the JSON object.
# - Last chunk, marking the end of the stream, contains the string "[DONE]".
#
# If the JSON object does not contain a `.choices[0].delta.content` field,
# assume it's a system/error message and return the entire object.
extract_streaming_content() {
  sed --unbuffered --regexp-extended \
    -e 's/^data: //g' \
    -e 's/\[DONE\]//g' \
    | jq --unbuffered --raw-output --join-output \
    'if .choices[0].delta != null then 
      .choices[0].delta.content // ""
     else 
       .
     end'
}

# Extract the content string from the non-streamed response.
#
# If the JSON object does not contain a `.choices[0].message.content` field,
# assume it's a system/error message and return the entire object.
extract_content() {
  jq --unbuffered --raw-output --join-output \
    '.choices[0].message.content // .'
}

# Transform SH41 shape to OpenAI's message schema.
# 
# From:
#  [{
#     role: "context" | "prompt" | "file" | "system_main-agent-mission" | "system_main-user-mission",
#     path: "string" | null,
#     content: "string"
#  }]
#
# To:
#  [{
#    role: "system" | "user",
#    content: "string"
#  }]
mutate() {
  jq \
   'def toOpenAIRole($role):
      if $role == "context" or $role == "file" or ($role | test("^system_")) 
      then "system"
      else $role
    end;

    . | map(
      {
        role: toOpenAIRole(.role // "user"),
        content: .content
      }
    )'
}

# ╭───┤ Input validation
# ╰─

should_stream="true"
should_output_raw=""
model=""
max_tokens=""
temperature="0.7"

while [ "$#" -gt 0 ]; do
  case $1 in
    --raw) should_output_raw="true" ;;
    --no-stream) should_stream="" ;;
    -mt|--max-tokens)
      guard_missing_option_value "-mt|--max-tokens" "$2"
      max_tokens=$2; shift
    ;;
    -t|--temperature)
      guard_missing_option_value "-t|--temperature" "$2"
      temperature=$2; shift
    ;;
    -m|--model)
      guard_missing_option_value "-m|--model" "$2"
      model=$2; shift
    ;;
    --) shift; break ;;
    -?*) guard_unknown_parameter "$1" ;;
    *) break ;;
  esac
  shift
done

guard_missing_argument "-m|--model" "$model"
guard_missing_argument "-mt|--max-tokens" "$max_tokens"

# ╭───┤ Main
# ╰─

if [ -n "$should_stream" ]; then
  curl_stream_flags="--no-buffer"
else
  # Workaround for curl throwing an error if an argument is empty. This ensures
  # curl_stream_flags is never empty, preventing "curl: option : blank argument
  # where content is expected".
  curl_stream_flags="--silent"
fi

log info \
  -v model "$model" \
  -v temperature "$temperature" \
  -v stream "$should_stream" \
  -v max_tokens "$max_tokens" \
  "OpenAI provider settings"

curl_stderr_tmp=$(mktemp -t sh41_openai-send_stderr-XXXXXX)
curl_exit_code_tmp=$(mktemp -t sh41_openai-send_exit-code-XXXXXX)

mutate < /dev/stdin \
 | jq --compact-output \
  --arg max_tokens "$max_tokens" \
  --arg model "$model" \
  --arg temperature "$temperature" \
  --arg stream "$should_stream" \
  '. as $messages | {
    model: $model,
    stream: ($stream | test("true")),
    temperature: ($temperature | tonumber),
    max_tokens: $max_tokens | tonumber,
    messages: $messages
  }' \
 | {
      curl \
      --silent "$curl_stream_flags" \
      --data @- \
      --show-error \
      --fail-with-body \
      --request POST \
      --header "Content-Type: application/json" \
      --header "Authorization: Bearer $OPENAI_API_KEY" \
      --header "OpenAI-Organization: $OPENAI_ORG_ID" \
        https://api.openai.com/v1/chat/completions 2> "$curl_stderr_tmp"
      echo $? > "$curl_exit_code_tmp"
   } \
  | if [ -n "$should_output_raw" ]; then
      cat
    elif [ -n "$should_stream" ]; then
      extract_streaming_content
    else
      extract_content
    fi

if [ "$(cat "$curl_exit_code_tmp")" -ne 0 ]; then
  log error "$(cat "$curl_stderr_tmp")"
  exit 4
fi
